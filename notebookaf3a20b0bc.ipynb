{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport torch\nimport nltk\nimport string\nimport numpy as np\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.nn import Linear, CrossEntropyLoss\nfrom sklearn.model_selection import train_test_split\nimport pickle\nfrom torch.nn import Linear\nfrom flask import Flask, render_template, request\nimport random\n\n\n# Download NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Initialize NLTK components\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words('english'))\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\n\n\n# Function to remove punctuation\ndef remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\n# Function for tokenization\ndef Tokenization(sentence):\n    return nltk.word_tokenize(sentence)\n\n\n# Function for stemming\ndef Stem(word):\n    return stemmer.stem(word.lower())\n\n\n# Function to remove stopwords\ndef RemoveStopwords(tokenized_sentence):\n    return [word for word in tokenized_sentence if word not in stop_words]\n\n\n# Load your dataset from CSV\nquestions = []\nlabels = []\n\nwith open('dataset_done_done_new.csv', 'r') as file:\n    reader = csv.reader(file)\n    next(reader)  # Skip the header row\n    for row in reader:\n        tag = row[1]\n        labels.append(tag)\n        for pattern in row[0].split(';'):\n            questions.append(pattern)\n\n\n\n\n\n# Text preprocessing\ncleaned_questions = []\nfor question in questions:\n    question = remove_punctuation(question)  # Remove punctuation\n    tokens = Tokenization(question)          # Tokenize\n    tokens = RemoveStopwords(tokens)         # Remove stopwords\n    cleaned_question = ' '.join(tokens)      # Re-join tokens into a single string\n    cleaned_questions.append(cleaned_question)\n\n\n# Encode questions\nencoded = tokenizer(questions, padding=True, truncation=True, return_tensors='pt')\n\n# Get embeddings for [CLS] token\nwith torch.no_grad():\n    outputs = model(**encoded)\n    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n\n# Prepare labels\nlabel_dict = {tag: index for index, tag in enumerate(set(labels))}\ny = torch.tensor([label_dict[label] for label in labels])\n\n# Split the dataset into training, validation, and test sets\ntrain_data, test_data, train_labels, test_labels = train_test_split(cls_embeddings, y, test_size=0.3, random_state=42)\nval_data, test_data, val_labels, test_labels = train_test_split(test_data, test_labels, test_size=0.6, random_state=42)\n\n# Convert the data into TensorDataset\ntrain_dataset = TensorDataset(train_data, train_labels)\nval_dataset = TensorDataset(val_data, val_labels)\ntest_dataset = TensorDataset(test_data, test_labels)\n\n# Data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n\n# Define a simple classifier\nn_classes = len(set(labels))\nclassifier = Linear(cls_embeddings.size(1), n_classes)\n\n# Training setup\noptimizer = torch.optim.Adam(classifier.parameters(), lr=0.01)\ncriterion = CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n\n# Save labels and questions using pickle\npickle.dump(labels, open('texts.pkl', 'wb'))\npickle.dump(questions, open('labels.pkl', 'wb'))\n\n# Training loop\nfor epoch in range(120):\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    # Training phase\n    classifier.train()\n    for inputs, targets in train_dataloader:\n        optimizer.zero_grad()\n        outputs = classifier(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == targets).sum().item()\n        total += targets.size(0)\n\n    # Validation phase\n    classifier.eval()\nval_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for inputs, targets in val_dataloader:\n            outputs = classifier(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            val_correct += (predicted == targets).sum().item()\n            val_total += targets.size(0)\n\n    train_accuracy = correct / total\n    val_accuracy = val_correct / val_total\n    print(\n        f'Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader):.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n\n# Evaluate on test set\nclassifier.eval()\ntest_correct = 0\ntest_total = 0\nwith torch.no_grad():\n    for inputs, targets in test_dataloader:\n        outputs = classifier(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        test_correct += (predicted == targets).sum().item()\n        test_total += targets.size(0)\n\ntest_accuracy = test_correct / test_total\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\n\n# Save the trained model\ntorch.save(classifier.state_dict(), 'roberta_classifier.h5')\n\n\n#flask code\napp = Flask(__name__)\napp.static_folder = 'static'\n\n# Load labels\nwith open('labels.pkl', 'rb') as f:\n    labels = pickle.load(f)\n\n\n# Load the trained model state_dict if available\nmodel_file = 'roberta_classifier.h5'\n\nif torch.cuda.is_available():\n    map_location = 'cuda:0'\nelse:\n    map_location = 'cpu'\n\ntry:\n    if torch.cuda.is_available():\n        state_dict = torch.load(model_file, map_location=map_location)\n    else:\n        state_dict = torch.load(model_file, map_location=torch.device('cpu'))\n\n    # Check if the state_dict needs adjustment due to output size mismatch\n    if 'weight' in state_dict and state_dict['weight'].shape != classifier.weight.shape:\n        print(f\"Adjusting the classifier output size from {classifier.out_features} to {state_dict['weight'].shape[0]}\")\n        classifier = Linear(model.config.hidden_size, state_dict['weight'].shape[0])\n\n    # Load the adjusted state_dict into the classifier\n    classifier.load_state_dict(state_dict)\n    classifier.eval()\n    print(\"Model loaded successfully!\")\nexcept FileNotFoundError:\n    print(f\"Error: Model file '{model_file}' not found.\")\nexcept Exception as e:\n    print(\"Error loading model:\", e)\n\n\ndef predict_class(sentence, threshold=0.5):\n    # Process the input sentence\n    encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n\n    # Get the CLS token embedding\n    with torch.no_grad():\n        outputs = model(**encoded_input)\n        cls_embedding = outputs.last_hidden_state[:, 0, :]\n\n    # Make a prediction\n    with torch.no_grad():\n        logits = classifier(cls_embedding)\n        probabilities = torch.softmax(logits, dim=1)\n        confidence, predicted_class = torch.max(probabilities, dim=1)  # torch.argmax(logits, dim=1).item()\n\n        if confidence.item() < threshold:\n            return \"I am Sorry There Is No Answer For This Question ðŸ˜¢ ðŸ’”\"\n        # predicted_class = torch.argmax(logits, dim=1).item()\n        predicted_label = list(label_dict.keys())[list(label_dict.values()).index(predicted_class)]\n\n    return predicted_label\n\n\n\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\")\n\n\n@app.route(\"/get\")\ndef get_bot_response():\n    userText = request.args.get('msg')\n    result=predict_class(userText)\n    return result\n\nif name == \"__main__\":\n    app.run(debug=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}